# Install the necessary libraries
!pip install transformers datasets

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import load_dataset

# Step 1: Load dataset
# For demonstration purposes, we assume the dataset has columns 'ingredients' and 'instructions'
# Replace this with the actual dataset you are working with.
dataset = load_dataset("recipe1m", split="train")

# Step 2: Tokenize the dataset
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Define a function to preprocess the data (convert ingredients and instructions to text)
def preprocess_function(examples):
    # Assuming ingredients is a list, and instructions are the recipe steps
    text = "Ingredients: " + ", ".join(examples["ingredients"]) + "\nInstructions: " + examples["instructions"]
    return tokenizer(text, padding="max_length", truncation=True, max_length=512)

tokenized_datasets = dataset.map(preprocess_function, batched=True)

# Step 3: Fine-tune GPT-2 model for recipe generation
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Set up the training arguments
training_args = TrainingArguments(
    output_dir="./recipe_model",  # Directory to store the trained model
    num_train_epochs=3,           # Number of training epochs
    per_device_train_batch_size=8,  # Batch size
    logging_dir="./logs",          # Directory to store logs
    logging_steps=500,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
)

# Train the model
trainer.train()

# Step 4: Generate inverse recipe (instructions) from ingredients
# Example input of ingredients
ingredients_input = "flour, sugar, eggs, butter, milk"

# Encode the input ingredients to prompt the model
input_ids = tokenizer.encode(f"Ingredients: {ingredients_input}\nInstructions:", return_tensors="pt")

# Generate recipe steps
model.eval()
with torch.no_grad():
    output_ids = model.generate(input_ids, max_length=200, num_beams=5, no_repeat_ngram_size=2, top_p=0.92, top_k=50)
    
# Decode the generated sequence to text
generated_recipe = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(generated_recipe)

